{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import parser\n",
    "import time\n",
    "\n",
    "from sunpy.net import Fido\n",
    "from sunpy.net.vso import attrs as avso\n",
    "from sunpy.time import TimeRange #parse_time,\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "\n",
    "import h5py\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfits(filename):\n",
    "    try:\n",
    "        ft = fits.open(filename, memmap=False)\n",
    "        hdr = ft[0].header\n",
    "        data = ft[0].data\n",
    "        axis1 = hdr['naxis1']\n",
    "        axis2 = hdr['naxis2']\n",
    "        ft.close()\n",
    "    \n",
    "    except ValueError:\n",
    "        axis1 = 1\n",
    "        axis2 = 2\n",
    "        data = None\n",
    "            \n",
    "    return axis1,axis2,data\n",
    "\n",
    "def writefits(filename, data, home_dir):\n",
    "    if not os.path.exists(f'{home_dir}{filename}.fits'):\n",
    "        fitsname = fits.PrimaryHDU(data)\n",
    "        fitsname.writeto(f'{home_dir}{filename}.fits')\n",
    "\n",
    "def holes(filename):\n",
    "    filename = str(filename)\n",
    "    \n",
    "    ft = fits.open(filename, memmap=False)\n",
    "    hdr = ft[0].header\n",
    "    data = ft[0].data\n",
    "    ft.close()\n",
    "\n",
    "    try:\n",
    "        x_coord = hdr['CRPIX1']\n",
    "        y_coord = hdr['CRPIX2']\n",
    "    \n",
    "    except KeyError:\n",
    "        x_coord = hdr['naxis1'] / 2.\n",
    "        y_coord = hdr['naxis2'] / 2.\n",
    "\n",
    "    y_ind,x_ind = np.indices((hdr['naxis1'],hdr['naxis2']))\n",
    "    rsquared = (x_ind - x_coord)**2 + (y_ind - y_coord)**2\n",
    "    \n",
    "    matches = ['96m', 'MDI']\n",
    "    \n",
    "    if 'efz' in filename: #good for all EIT products \n",
    "        rad = x_coord*np.sqrt(2)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "\n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "    \n",
    "    elif any([x in filename for x in matches]):\n",
    "        rad1 = float(x_coord)\n",
    "        rad2 = 0.6*float(x_coord)\n",
    "        indices_rad1 = np.where(rsquared.flatten() < rad1**2)[0]\n",
    "        indices_rad2 = np.where(rsquared.flatten() < rad2**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices_rad1] == 0.)[0]\n",
    "        nan_ind = np.where(data.flatten()[indices_rad2] != data.flatten()[indices_rad2])[0]\n",
    "        zeros_nan_ind_len = len(list(zeros_ind) + list(nan_ind))\n",
    "        \n",
    "        if zeros_nan_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "\n",
    "    elif 'LASCO_C3' in filename:\n",
    "        #print('LASCO_C3')\n",
    "        rad = 0.8*x_coord\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)  \n",
    "        \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image   \n",
    "\n",
    "    \n",
    "    elif 'LASCO_C2' in filename:\n",
    "        #print('LASCO_C2')\n",
    "        rad1 = 160 #this seems good\n",
    "        #print('rad1:', rad1)\n",
    "        rad2 = int(x_coord)\n",
    "        indices = np.where((rad2**2 > rsquared.flatten()) & (rsquared.flatten() > rad1**2))[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "     \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "        \n",
    "\n",
    "def data_reducer(data,flag,target_dimension,axis1_shape):\n",
    "    scale_factor = int(axis1_shape/target_dimension)\n",
    "    \n",
    "    if flag == 'subsample':\n",
    "        reduced_data = data[::scale_factor].T[::scale_factor].T #subsampling image; every other row,column\n",
    "    elif flag == 'interp': #linear interpolation with anti_aliasing and range preserving\n",
    "        reduced_data = rescale(data, (1/scale_factor), order=1, anti_aliasing=True, preserve_range=True)\n",
    "    elif flag == 'minpool': #min pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.min)\n",
    "    elif flag == 'maxpool': #max pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.max)\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "def prev_time_resumer(home_dir, base, time_range_orig): \n",
    "    #CAN RE-RUN PROGRAM FROM THE LAST DATE ON WHICH STOPPED; WILL PICK UP THE TIMES THAT ARE PRESENT AND CONTINUE!\n",
    "    # CHECKS WHETHER THE START DAY THAT ENTERED IS ALREADY CONTAINED IN THE FILES OF PREVIOUS DAY AND START_DATE FROM THAT EXACT TIME! \n",
    "    # ALSO THIS WORKS IF START ON A NEW DAY AND ARE LOOKING BACK ON THE PREVIOUS DAY.\n",
    "    \n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre)\n",
    "    \n",
    "    if len(data_files) != 0:\n",
    "        prev_time_pre = data_files[-1] \n",
    "        prev_time = [prev_time_pre.split('_')[3]]\n",
    "             \n",
    "        time_orig_pre = str(time_range_orig.start)\n",
    "        time_orig = ''.join(time_orig_pre.split(' ')[0].split('-'))\n",
    "        \n",
    "        if str(prev_time[0][0:8]) == time_orig:\n",
    "            time_begin = prev_time[0]\n",
    "            time_range = TimeRange(time_begin, time_range_orig.end)\n",
    "        else:\n",
    "            time_range = time_range_orig            \n",
    "    \n",
    "    elif len(data_files) == 0:\n",
    "        prev_time = []\n",
    "        time_range = time_range_orig   \n",
    "    \n",
    "    return prev_time, time_range\n",
    "\n",
    "\n",
    "def data_name_selector(home_dir, base, date_start, date_finish):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre)\n",
    "    print('len(data_files):', len(data_files)) \n",
    "    \n",
    "    if len(data_files) != 0: \n",
    "        time_start_name_pre = data_files[0] \n",
    "        time_finish_name_pre = data_files[-1]\n",
    "        \n",
    "        if 'EIT' in str(time_start_name_pre):\n",
    "            time_start_name = str(time_start_name_pre.split('_')[2])\n",
    "            time_finish_name = str(time_finish_name_pre.split('_')[2])        \n",
    "        else:         \n",
    "            time_start_name = str(time_start_name_pre.split('_')[3])\n",
    "            time_finish_name = str(time_finish_name_pre.split('_')[3])\n",
    "            \n",
    "        time_start_name_new = time_start_name[0:4] + '-' + time_start_name[4:6] + '-' + time_start_name[6:8] + '-' + time_start_name[8:10] + ':' + time_start_name[10:12] + ':' + time_start_name[12:14]\n",
    "        time_finish_name_new = time_finish_name[0:4] + '-' + time_finish_name[4:6] + '-' + time_finish_name[6:8] + '-' + time_finish_name[8:10] + ':' + time_finish_name[10:12] + ':' + time_finish_name[12:14]\n",
    "    \n",
    "    else: \n",
    "        time_start_name_new = date_start \n",
    "        time_finish_name_new = date_finish  \n",
    "        \n",
    "    return time_start_name_new, time_finish_name_new\n",
    "\n",
    "\n",
    "def data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre) #to have chronological order and to sink order with list of individual product times\n",
    "    print('len(data_files):', len(data_files))\n",
    "    \n",
    "    data_content_list = []\n",
    "    for elem in data_files:\n",
    "        axdim1,axdim2,data_content = readfits(f'{filepath}{elem}')\n",
    "        if (axdim1 == axdim2) and ('SOHO' in elem):\n",
    "            data_content_list.append(data_content)\n",
    "\n",
    "    if data_content_list:\n",
    "        data_content_stack = np.stack(data_content_list)\n",
    "    else:\n",
    "        data_content_stack = []\n",
    "    \n",
    "    time_start_name_new, time_finish_name_new = data_name_selector(home_dir, base, date_start, date_finish)\n",
    "    \n",
    "    data_content_stack = np.stack(data_content_list)\n",
    "    data_cube = h5py.File(f'{home_dir}{time_start_name_new}_to_{time_finish_name_new}_{base}_{flag}_{target_dimension}.h5', 'w')\n",
    "    data_cube.create_dataset(f'{base}_{target_dimension}', data=data_content_stack, compression=\"gzip\")\n",
    "    data_cube.close()\n",
    "                            \n",
    "    return data_cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_search(base,time_range,date_time_start):\n",
    "    if 'EIT' in base:\n",
    "        wavelen = int(base[3:6])\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('EIT'),avso.Provider('SDAC'),avso.Wavelength(wavelen * avso.u.Angstrom, wavelen * avso.u.Angstrom))\n",
    "    \n",
    "    elif 'MDI' in base:\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('MDI'),avso.Provider('SDAC'),avso.Physobs('LOS_MAGNETIC_FIELD'))\n",
    "    \n",
    "    elif 'LASCO' in base:\n",
    "        detector = base.split('_')[1]\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Provider('SDAC'),avso.Source('SOHO'),avso.Instrument('LASCO'),avso.Detector(detector))\n",
    "    \n",
    "    return product_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_sizes(base,product_results):\n",
    "    \n",
    "    matches = ['171', '304', '284']\n",
    "    \n",
    "    if 'EIT195' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind_2059 = np.where(np.array(size_list) == 2059)[0]\n",
    "        ind_523 = np.where(np.array(size_list) == 523)[0]\n",
    "        print(len(ind_2059))\n",
    "        print(len(ind_523))\n",
    "        ind = np.sort(list(ind_2059) + list(ind_523)) #important to sort here since combining two lists!\n",
    "        print(len(ind))\n",
    "        \n",
    "    elif 'MDI' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 4115.0)[0]\n",
    "        print(len(ind))        \n",
    "        \n",
    "    elif 'LASCO' in base:\n",
    "        size_list = [int(np.ceil(elem['size'] / 100.0))*100 for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2100.0)[0] \n",
    "        print(len(ind))\n",
    "        \n",
    "    elif any([x in base for x in matches]):\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2059)[0]        \n",
    "        print(len(ind))\n",
    "        \n",
    "    return ind\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indices(base,ind,product_results,time_window,look_ahead,prev_time):\n",
    "    \n",
    "    all_size_sieved_times_pre = [] #local list to populate at each loop\n",
    "    all_time_window_sieved_times = [] #local list to populate at each loop\n",
    "\n",
    "    for value in ind:\n",
    "        all_size_sieved_times_pre.append(product_results.get_response(0)[int(value)]['time']['start'])\n",
    "    all_size_sieved_times = list(np.unique(all_size_sieved_times_pre))\n",
    "    all_size_sieved_times_aug = prev_time + all_size_sieved_times\n",
    "\n",
    "    for i,time_value in enumerate(all_size_sieved_times_aug):\n",
    "        local_time_range = TimeRange(str(time_value),timedelta(hours=time_window))\n",
    "\n",
    "        local_list = []\n",
    "        for k,time_val in enumerate(all_size_sieved_times_aug[i:i+look_ahead]):\n",
    "            if time_val in local_time_range:\n",
    "                local_list.append(time_val)\n",
    "        if local_list:\n",
    "            for entry in local_list[1:]:\n",
    "                all_size_sieved_times_aug.remove(entry)\n",
    "            all_time_window_sieved_times.append(local_list[0])\n",
    "\n",
    "    all_time_window_sieved_times_product_times = list(np.unique(all_time_window_sieved_times)) #np.unique() does np.array() and np.sort()\n",
    "\n",
    "    if not prev_time:\n",
    "        all_time_window_sieved_times_product_times_inds_list_pre = [np.where(np.array(all_size_sieved_times_pre) == item)[0][0] for item in all_time_window_sieved_times_product_times]\n",
    "        new_inds = [np.where(np.array(all_size_sieved_times_pre) == entry)[0][0] for entry in all_time_window_sieved_times_product_times]           \n",
    "    elif prev_time:\n",
    "        all_time_window_sieved_times_product_times_inds_list_pre = [np.where(np.array(all_size_sieved_times_pre) == item)[0][0] for item in all_time_window_sieved_times_product_times[1:]]        \n",
    "        new_inds = [np.where(np.array(all_size_sieved_times_pre) == entry)[0][0] for entry in all_time_window_sieved_times_product_times[1:]]    \n",
    "    \n",
    "    if all_time_window_sieved_times_product_times_inds_list_pre:\n",
    "        all_time_window_sieved_times_product_times_inds_list = list(np.hstack(all_time_window_sieved_times_product_times_inds_list_pre))\n",
    "    else:\n",
    "        all_time_window_sieved_times_product_times_inds_list = []\n",
    "        \n",
    "    fetch_indices_product = ind[new_inds]\n",
    "    \n",
    "    return all_size_sieved_times_pre, all_time_window_sieved_times_product_times, all_time_window_sieved_times_product_times_inds_list, fetch_indices_product\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_retriever(base,product_results,indiv_ind,url_prefix,home_dir):\n",
    "    \n",
    "    fileid = product_results.get_response(0)[int(indiv_ind)]['fileid']\n",
    "    item_wget =  url_prefix + fileid\n",
    "    cmd = 'wget' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}' #OBTAIN TIMEOUT ISSUE WITH FIDO FETCH! SEEMS THAT WITH WGET CAN CIRCUMNAVIGATE IT.\n",
    "    args = shlex.split(cmd)\n",
    "    wget_output = str(subprocess.check_output(args)).strip('b')\n",
    "    \n",
    "    while wget_output != \"''\":\n",
    "        print(f'Encountered wget error with exit status {wget_output} for {item_wget}')\n",
    "        cmd = 'wget' + ' ' + '--retry-connrefused' + ' ' + '--waitretry=1' + ' ' + '--read-timeout=20' + ' ' + '--timeout=15' + ' ' + '-t' + ' ' + '0' + ' ' + '--continue' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}'    \n",
    "        args = shlex.split(cmd)\n",
    "        wget_output = str(subprocess.check_output(args)).strip('b')\n",
    "        if wget_output == \"''\":\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        \n",
    "    downloaded_fileid = fileid.split('/')[-1]\n",
    "    query_result = [f'{home_dir}{base}/{downloaded_fileid}']\n",
    "    \n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_distiller(base, axis1_product,axis2_product,data_product, all_size_sieved_times_pre, all_time_window_sieved_times_product_times, all_time_window_sieved_times_product_times_inds_list, query_result, ind, indiv_ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir):\n",
    "\n",
    "    holes_product_list = []\n",
    "    unreadable_file_ids_product_list = []\n",
    "    \n",
    "    if (data_product is not None) and (axis1_product == axis2_product):\n",
    "\n",
    "        if not holes(query_result[0]): #so if not True; so no holes; can use image\n",
    "            reduced_product_data = data_reducer(data_product,flag,target_dimension,axis1_product)\n",
    "            time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "            os.remove(query_result[0]) #delete original downloaded file\n",
    "\n",
    "        elif holes(query_result[0]): #so if True, if there are holes\n",
    "            time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start'] \n",
    "            hole_loc = url_prefix + product_results.get_response(0)[int(indiv_ind)]['fileid']                       \n",
    "            holes_product_list.append((hole_loc, str(time_data)))\n",
    "            hole_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            \n",
    "            all_time_window_sieved_times_product_times.remove(hole_time_val)\n",
    "            \n",
    "            ind_hole_time_val = np.where(np.array(all_size_sieved_times_pre) == hole_time_val)[0][0]\n",
    "            \n",
    "            all_time_window_sieved_times_product_times_inds_list.remove(ind_hole_time_val)\n",
    "\n",
    "            os.remove(query_result[0]) #delete original downloaded file\n",
    "            ind_timespickup = np.where(np.array(all_size_sieved_times_pre) == hole_time_val)[0][0]\n",
    "            zoomed_time_range = TimeRange(str(hole_time_val),timedelta(hours=time_window))\n",
    "\n",
    "            fetch_inds_to_try_list = [] \n",
    "            #the zeroth entry didn't have it so that's why plus 1 in the brackets\n",
    "            for time_val in all_size_sieved_times_pre[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                    ind_next_good_time = np.where(np.array(all_size_sieved_times_pre) == time_val)[0][0]\n",
    "                    fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                    fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "            for index in fetch_inds_to_try_list:\n",
    "                query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                if (data_next_good is not None) and (axis1_next_good == axis2_next_good):\n",
    "\n",
    "                    if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                        reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                        all_time_window_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                        all_time_window_sieved_times_product_times_inds_list.append(index)\n",
    "                        os.remove(query_result_next[0]) #delete original downloaded file\n",
    "                        break\n",
    "\n",
    "                    elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                        holes_product_list.append((hole_loc, str(time_data)))\n",
    "                        os.remove(query_result_next[0])\n",
    "                        continue \n",
    "\n",
    "                elif (data_next_good is None) or (axis1_next_good != axis2_next_good):\n",
    "                    unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue #continue the for loop\n",
    "\n",
    "\n",
    "    elif (data_product is None) or (axis1_product != axis2_product):\n",
    "        unreadable_file_ids_product_list.append(product_results.get_response(0)[int(indiv_ind)]['fileid'])\n",
    "        bad_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "        all_time_window_sieved_times_product_times.remove(bad_time_val)\n",
    "        ind_bad_time_val = np.where(np.array(all_size_sieved_times_pre) == bad_time_val)[0][0]\n",
    "        all_time_window_sieved_times_product_times_inds_list.remove(ind_bad_time_val)\n",
    "        os.remove(query_result[0]) #delete original downloaded file\n",
    "        ind_timespickup = np.where(np.array(all_size_sieved_times_pre) == bad_time_val)[0][0]\n",
    "        zoomed_time_range = TimeRange(str(bad_time_val),timedelta(hours=time_window))\n",
    "\n",
    "        fetch_inds_to_try_list = [] #gets reset for each new item\n",
    "        for time_val in all_size_sieved_times_pre[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "            if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                ind_next_good_time = np.where(np.array(all_size_sieved_times_pre) == time_val)[0][0]\n",
    "                fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "        for index in fetch_inds_to_try_list:\n",
    "            query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "            axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "            if (data_next_good is not None) and (axis1_next_good == axis2_next_good):\n",
    "\n",
    "                if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                    reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                    time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                    writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                    all_time_window_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                    all_time_window_sieved_times_product_times_inds_list.append(index)\n",
    "                    os.remove(query_result_next[0])\n",
    "                    break\n",
    "\n",
    "                elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                    time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                    hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                    holes_product_list.append((hole_loc, str(time_data)))\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue \n",
    "\n",
    "            elif (data_next_good is None) or (axis1_next_good != axis2_next_good):\n",
    "                unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                os.remove(query_result_next[0])\n",
    "                continue\n",
    "    \n",
    "    all_time_window_sieved_times_product_times_modified = all_time_window_sieved_times_product_times\n",
    "\n",
    "    return all_time_window_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list \n",
    "    #think whether need this new name or need to feed back, i think is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_time_window_sieved_times_sorted):\n",
    "    with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv', 'a') as f: #appending lines so not overwriting the file\n",
    "        writer = csv.writer(f, delimiter='\\n')\n",
    "        writer.writerow(all_time_window_sieved_times_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def main(date_start, date_finish, target_dimension, time_increment, time_window, flag, home_dir, bases):''' \n",
    "    \n",
    "date_start = '1996-12-01' #'1996-01-01'\n",
    "date_finish = '1996-12-03' #'2011-05-01'\n",
    "target_dimension = 128\n",
    "time_increment = 1 #60\n",
    "time_window = 2 #6\n",
    "flag = 'subsample'\n",
    "home_dir = '/home/carl/Documents/'\n",
    "bases = 'EIT195, MDI_96m, LASCO_C2, LASCO_C3, EIT171, EIT304, EIT284' #or a subset of these products\n",
    "\n",
    "date_time_pre_start = date_start + '-0000'\n",
    "date_time_start= parser.parse(date_time_pre_start)\n",
    "print('date_time_start:', date_time_start)\n",
    "\n",
    "date_time_pre_end = date_finish + '-2359'\n",
    "date_time_end = parser.parse(date_time_pre_end)\n",
    "print('date_time_end:', date_time_end)\n",
    "\n",
    "target_dimension = int(target_dimension)\n",
    "print('target_dimension:', target_dimension)\n",
    "\n",
    "time_increment = int(time_increment)\n",
    "time_window = float(time_window)\n",
    "\n",
    "flag = str(flag) \n",
    "print('flag:', flag)\n",
    "\n",
    "home_dir = str(home_dir)\n",
    "print('home_dir:', home_dir)\n",
    "\n",
    "url_prefix = 'https://seal.nascom.nasa.gov/'\n",
    "print('url_prefix:', url_prefix)\n",
    "\n",
    "look_ahead = int(np.ceil(time_window*60/10)) #should sufficiently cover all 7 products based on their cadence.\n",
    "print('look_ahead:', look_ahead)\n",
    "\n",
    "diff_start_finish_total_sec = (date_time_end - date_time_start).total_seconds()\n",
    "print('diff_start_finish_total_sec:', diff_start_finish_total_sec)\n",
    "\n",
    "total_sec = timedelta(days = time_increment).total_seconds()\n",
    "print('total_sec:', total_sec)\n",
    "\n",
    "num_loops = np.ceil(diff_start_finish_total_sec/total_sec) + 1 #num_loops would be equal to 94 + 1 for 19960101-0000' - '20110501-0000'; discete number of loops so go over rather than under\n",
    "print('num_loops:', num_loops)\n",
    "\n",
    "base_list = bases.split(',')\n",
    "for base in base_list:\n",
    "    base = base.strip(' ')\n",
    "    holes_list = []\n",
    "    unreadable_file_ids_product_list_global = []\n",
    "    \n",
    "    print(f'***{base}***')\n",
    "    base_dir = home_dir + base\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)    \n",
    "\n",
    "    time_range = TimeRange(date_time_start, timedelta(days = time_increment)) #time_range re-initialized here\n",
    "    #print('time_range:', time_range)\n",
    "\n",
    "    prev_time, time_range_modified = prev_time_resumer(home_dir, base, time_range)\n",
    "    for t_value in np.arange(num_loops): #main workhorse loop\n",
    "        print('t_value:', t_value)\n",
    "        print('prev_time:', prev_time)\n",
    "        \n",
    "        if time_range_modified.end > date_time_end:\n",
    "            time_range_modified = TimeRange(time_range_modified.start, date_time_end)\n",
    "            \n",
    "        product_results = product_search(base,time_range_modified,date_time_start)\n",
    "        product_results_number = product_results.file_num\n",
    "        if product_results_number != 0:\n",
    "            ind = index_of_sizes(base,product_results)\n",
    "            all_size_sieved_times_pre, all_time_window_sieved_times_product_times, all_time_window_sieved_times_product_times_inds_list, fetch_indices_product = fetch_indices(base,ind,product_results,time_window,look_ahead,prev_time)          \n",
    "            \n",
    "            if len(fetch_indices_product) != 0:\n",
    "                for item in fetch_indices_product:\n",
    "                    query_result = product_retriever(base,product_results,item,url_prefix,home_dir)\n",
    "                    axis1_product,axis2_product,data_product = readfits(query_result[0])\n",
    "                    all_time_window_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list_local = product_distiller(base, axis1_product,axis2_product,data_product, all_size_sieved_times_pre, all_time_window_sieved_times_product_times, all_time_window_sieved_times_product_times_inds_list, query_result, ind, item, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir)\n",
    "\n",
    "                    if holes_product_list:\n",
    "                        holes_list.append(holes_product_list)\n",
    "\n",
    "                    if unreadable_file_ids_product_list_local:\n",
    "                        unreadable_file_ids_product_list_global.append(unreadable_file_ids_product_list_local)\n",
    "\n",
    "                if not prev_time:\n",
    "                    all_time_window_sieved_times_sorted = np.unique(all_time_window_sieved_times_product_times_modified)\n",
    "                elif prev_time:\n",
    "                    all_time_window_sieved_times_sorted = np.unique(all_time_window_sieved_times_product_times_modified[1:])                \n",
    "\n",
    "                print(f'{base} np.unique(all_size_sieved_times_pre):', np.unique(all_size_sieved_times_pre), len(np.unique(all_size_sieved_times_pre)))\n",
    "                print(f'{base} list(all_time_window_sieved_times_sorted):', list(all_time_window_sieved_times_sorted), len(all_time_window_sieved_times_sorted))\n",
    "\n",
    "                prev_time = [] #reset to empty list\n",
    "                if len(all_time_window_sieved_times_sorted) != 0:\n",
    "                    prev_time.append(all_time_window_sieved_times_sorted[-1])\n",
    "\n",
    "                csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_time_window_sieved_times_sorted)\n",
    "\n",
    "            else:\n",
    "                holes_list = []\n",
    "                unreadable_file_ids_product_list_global = []\n",
    "        \n",
    "        time_range_modified.next() #Sunpy iterator to go for the next 2 months #also have time_range_modified.previous() to go back.    \n",
    "        #print('time_range_modified next:', time_range_modified)\n",
    "        \n",
    "    print(f'{base} holes_list', holes_list)\n",
    "    print(f'{base} unreadable_file_ids_product_list_global:', unreadable_file_ids_product_list_global)\n",
    "\n",
    "    data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
