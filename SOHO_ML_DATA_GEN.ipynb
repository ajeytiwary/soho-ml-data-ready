{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "from sunpy.net import Fido\n",
    "from sunpy.net.vso import attrs as avso\n",
    "from sunpy.time import TimeRange #parse_time,\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "\n",
    "import h5py\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfits(filename):\n",
    "    try:\n",
    "        ft = fits.open(filename, memmap=False)\n",
    "        hdr = ft[0].header\n",
    "        data = ft[0].data\n",
    "        axis1 = hdr['naxis1']\n",
    "        axis2 = hdr['naxis2']\n",
    "        ft.close()\n",
    "    \n",
    "    except ValueError:\n",
    "        axis1 = 1\n",
    "        axis2 = 2\n",
    "        data = None\n",
    "            \n",
    "    return axis1,axis2,data\n",
    "\n",
    "def writefits(filename, data, home_dir):\n",
    "    if not os.path.exists(f'{home_dir}{filename}.fits'):\n",
    "        fitsname = fits.PrimaryHDU(data)\n",
    "        fitsname.writeto(f'{home_dir}{filename}.fits')\n",
    "\n",
    "def holes(filename):\n",
    "    filename = str(filename)\n",
    "    \n",
    "    ft = fits.open(filename, memmap=False)\n",
    "    hdr = ft[0].header\n",
    "    data = ft[0].data\n",
    "    ft.close()\n",
    "\n",
    "    try:\n",
    "        x_coord = hdr['CRPIX1']\n",
    "        y_coord = hdr['CRPIX2']\n",
    "    \n",
    "    except KeyError:\n",
    "        x_coord = hdr['naxis1'] / 2.\n",
    "        y_coord = hdr['naxis2'] / 2.\n",
    "\n",
    "    y_ind,x_ind = np.indices((hdr['naxis1'],hdr['naxis2']))\n",
    "    rsquared = (x_ind - x_coord)**2 + (y_ind - y_coord)**2\n",
    "    \n",
    "    matches = ['96m', 'MDI']\n",
    "    \n",
    "    if 'efz' in filename: #good for all EIT products \n",
    "        rad = x_coord*np.sqrt(2)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "\n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "    \n",
    "    elif any([x in filename for x in matches]):\n",
    "        rad = float(x_coord)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        import numpy as np\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "from sunpy.net import Fido\n",
    "from sunpy.net.vso import attrs as avso\n",
    "from sunpy.time import TimeRange #parse_time,\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "\n",
    "import h5py\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def readfits(filename):\n",
    "    try:\n",
    "        ft = fits.open(filename, memmap=False)\n",
    "        hdr = ft[0].header\n",
    "        data = ft[0].data\n",
    "        axis1 = hdr['naxis1']\n",
    "        axis2 = hdr['naxis2']\n",
    "        ft.close()\n",
    "    \n",
    "    except ValueError:\n",
    "        axis1 = 1\n",
    "        axis2 = 2\n",
    "        data = None\n",
    "            \n",
    "    return axis1,axis2,data\n",
    "\n",
    "def writefits(filename, data, home_dir):\n",
    "    if not os.path.exists(f'{home_dir}{filename}.fits'):\n",
    "        fitsname = fits.PrimaryHDU(data)\n",
    "        fitsname.writeto(f'{home_dir}{filename}.fits')\n",
    "\n",
    "def holes(filename):\n",
    "    filename = str(filename)\n",
    "    \n",
    "    ft = fits.open(filename, memmap=False)\n",
    "    hdr = ft[0].header\n",
    "    data = ft[0].data\n",
    "    ft.close()\n",
    "\n",
    "    try:\n",
    "        x_coord = hdr['CRPIX1']\n",
    "        y_coord = hdr['CRPIX2']\n",
    "    \n",
    "    except KeyError:\n",
    "        x_coord = hdr['naxis1'] / 2.\n",
    "        y_coord = hdr['naxis2'] / 2.\n",
    "\n",
    "    y_ind,x_ind = np.indices((hdr['naxis1'],hdr['naxis2']))\n",
    "    rsquared = (x_ind - x_coord)**2 + (y_ind - y_coord)**2\n",
    "    \n",
    "    matches = ['96m', 'MDI']\n",
    "    \n",
    "    if 'efz' in filename: #good for all EIT products \n",
    "        rad = x_coord*np.sqrt(2)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "\n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "    \n",
    "    elif any([x in filename for x in matches]):\n",
    "        rad = float(x_coord)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        nan_ind = np.where(data.flatten()[indices] != data.flatten()[indices])[0]\n",
    "        zeros_nan_ind_len = len(list(zeros_ind) + list(nan_ind))\n",
    "        \n",
    "        if zeros_nan_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "\n",
    "    elif 'LASCO_C3' in filename:\n",
    "        #print('LASCO_C3')\n",
    "        rad = 0.8*x_coord\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)  \n",
    "        \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image   \n",
    "\n",
    "    \n",
    "    elif 'LASCO_C2' in filename:\n",
    "        #print('LASCO_C2')\n",
    "        rad1 = 160 #this seems good\n",
    "        #print('rad1:', rad1)\n",
    "        rad2 = int(x_coord)\n",
    "        indices = np.where((rad2**2 > rsquared.flatten()) & (rsquared.flatten() > rad1**2))[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "     \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "        \n",
    "\n",
    "def data_reducer(data,flag,target_dimension,axis1_shape):\n",
    "    scale_factor = int(axis1_shape/target_dimension)\n",
    "    \n",
    "    if flag == 'subsample':\n",
    "        reduced_data = data[::scale_factor].T[::scale_factor].T #subsampling image; every other row,column\n",
    "    elif flag == 'interp': #linear interpolation with anti_aliasing and range preserving\n",
    "        reduced_data = rescale(data, (1/scale_factor), order=1, anti_aliasing=True, preserve_range=True)\n",
    "    elif flag == 'minpool': #min pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.min)\n",
    "    elif flag == 'maxpool': #max pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.max)\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "def data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre) #to have chronological order and to sink order with list of individual product times\n",
    "    print('len(data_files):', len(data_files))\n",
    "    \n",
    "    data_content_list = []\n",
    "    for elem in data_files:\n",
    "        axdim1,axdim2,data_content = readfits(f'{filepath}{elem}')\n",
    "        data_content_list.append(data_content)\n",
    "\n",
    "    data_content_stack = np.stack(data_content_list)\n",
    "    data_cube = h5py.File(f'{home_dir}{date_start}_to_{date_finish}_{base}_{flag}_{target_dimension}.h5', 'w')\n",
    "    data_cube.create_dataset(f'{base}_{target_dimension}', data=data_content_stack, compression=\"gzip\")\n",
    "    data_cube.close()\n",
    "                            \n",
    "    return data_cube\n",
    "\n",
    "\n",
    "def product_search(base,time_range,date_time_start):\n",
    "    if 'EIT' in base:\n",
    "        wavelen = int(base[3:6])\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('EIT'),avso.Provider('SDAC'),avso.Wavelength(wavelen * avso.u.Angstrom, wavelen * avso.u.Angstrom))\n",
    "    \n",
    "    elif 'MDI' in base:\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('MDI'),avso.Provider('SDAC'),avso.Physobs('LOS_MAGNETIC_FIELD'))\n",
    "    \n",
    "    elif 'LASCO' in base:\n",
    "        detector = base.split('_')[1]\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Provider('SDAC'),avso.Source('SOHO'),avso.Instrument('LASCO'),avso.Detector(detector))\n",
    "    \n",
    "    return product_results\n",
    "\n",
    "\n",
    "def index_of_sizes(base,product_results):\n",
    "    \n",
    "    matches = ['171', '304', '284']\n",
    "    \n",
    "    if 'EIT195' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind_2059 = np.where(np.array(size_list) == 2059)[0]\n",
    "        ind_523 = np.where(np.array(size_list) == 523)[0]\n",
    "        print(len(ind_2059))\n",
    "        print(len(ind_523))\n",
    "        ind = np.sort(list(ind_2059) + list(ind_523)) #important to sort here since combining two lists!\n",
    "        print(len(ind))\n",
    "        \n",
    "    elif 'MDI' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 4115.0)[0]\n",
    "        print(len(ind))        \n",
    "        \n",
    "    elif 'LASCO' in base:\n",
    "        size_list = [int(np.ceil(elem['size'] / 100.0))*100 for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2100.0)[0] \n",
    "        print(len(ind))\n",
    "        \n",
    "    elif any([x in base for x in matches]):\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2059)[0]        \n",
    "        print(len(ind))\n",
    "        \n",
    "    return ind\n",
    "   \n",
    "\n",
    "def fetch_indices(base,ind,product_results,time_window,look_ahead):\n",
    "    \n",
    "    all_size_sieved_times = [] #local list to populate at each loop\n",
    "    all_2hr_sieved_times = [] #local list to populate at each loop\n",
    "\n",
    "    for value in ind:\n",
    "        all_size_sieved_times.append(product_results.get_response(0)[int(value)]['time']['start'])\n",
    "    all_size_sieved_times_copy = all_size_sieved_times.copy()\n",
    "\n",
    "    for i,time_value in enumerate(all_size_sieved_times_copy):\n",
    "        local_time_range = TimeRange(str(time_value),timedelta(hours=time_window))\n",
    "\n",
    "        local_list = []\n",
    "        for k,time_val in enumerate(all_size_sieved_times_copy[i:i+look_ahead]):\n",
    "            if time_val in local_time_range:\n",
    "                local_list.append(time_val)\n",
    "        if local_list:\n",
    "            for entry in local_list[1:]:\n",
    "                all_size_sieved_times_copy.remove(entry)\n",
    "            all_2hr_sieved_times.append(local_list[0])\n",
    "\n",
    "    all_2hr_sieved_times_product_times = list(np.unique(all_2hr_sieved_times)) #np.unique() does np.array() and np.sort()\n",
    "\n",
    "    all_2hr_sieved_times_product_times_inds_list_pre = [np.where(np.array(all_size_sieved_times) == item)[0] for item in all_2hr_sieved_times_product_times]\n",
    "    \n",
    "    if all_2hr_sieved_times_product_times_inds_list_pre:\n",
    "        all_2hr_sieved_times_product_times_inds_list = list(np.hstack(all_2hr_sieved_times_product_times_inds_list_pre))\n",
    "\n",
    "    fetch_indices_product = ind[all_2hr_sieved_times_product_times_inds_list] #THESE ARE THE INDICES TO FETCH!\n",
    "    \n",
    "    return all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, fetch_indices_product\n",
    "    \n",
    "\n",
    "def product_retriever(base,product_results,indiv_ind,url_prefix,home_dir):\n",
    "    \n",
    "    fileid = product_results.get_response(0)[int(indiv_ind)]['fileid']\n",
    "    item_wget =  url_prefix + fileid\n",
    "    cmd = 'wget' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}' #OBTAIN TIMEOUT ISSUE WITH FIDO FETCH! SEEMS THAT WITH WGET CAN CIRCUMNAVIGATE IT.\n",
    "    args = shlex.split(cmd)\n",
    "    subprocess.check_call(args)\n",
    "    downloaded_fileid = fileid.split('/')[-1]\n",
    "    query_result = [f'{home_dir}{base}/{downloaded_fileid}']\n",
    "    \n",
    "    return query_result\n",
    "\n",
    "\n",
    "def product_distiller(base, axis1_product,axis2_product,data_product, all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, query_result, ind, indiv_ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir):\n",
    "\n",
    "    holes_product_list = []\n",
    "    unreadable_file_ids_product_list = []\n",
    "    \n",
    "    if (data_product is not None) and (axis1_product == axis2_product):\n",
    "\n",
    "        if not holes(query_result[0]): #so if not True; so no holes; can use image\n",
    "            reduced_product_data = data_reducer(data_product,flag,target_dimension,axis1_product)\n",
    "            time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "            os.remove(query_result[0]) #delete original downloaded file\n",
    "\n",
    "        elif holes(query_result[0]): #so if True, if there are holes\n",
    "            time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start'] \n",
    "            hole_loc = url_prefix + product_results.get_response(0)[int(indiv_ind)]['fileid']                       \n",
    "            holes_product_list.append((hole_loc, str(time_data)))\n",
    "            hole_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            \n",
    "            all_2hr_sieved_times_product_times.remove(hole_time_val)\n",
    "            \n",
    "            ind_hole_time_val = np.where(np.array(all_size_sieved_times) == hole_time_val)[0][0]\n",
    "            \n",
    "            all_2hr_sieved_times_product_times_inds_list.remove(ind_hole_time_val)\n",
    "\n",
    "            os.remove(query_result[0]) #delete original downloaded file\n",
    "            ind_timespickup = np.where(np.array(all_size_sieved_times) == hole_time_val)[0][0]\n",
    "            zoomed_time_range = TimeRange(str(hole_time_val),timedelta(hours=time_window))\n",
    "\n",
    "            fetch_inds_to_try_list = [] \n",
    "            #the zeroth entry didn't have it so that's why plus 1 in the brackets\n",
    "            for time_val in all_size_sieved_times[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                    ind_next_good_time = np.where(np.array(all_size_sieved_times) == time_val)[0]\n",
    "                    fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                    fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "            for index in fetch_inds_to_try_list:\n",
    "                query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                if data_next_good is not None and axis1_next_good == axis2_next_good:\n",
    "\n",
    "                    if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                        reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                        all_2hr_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                        all_2hr_sieved_times_product_times_inds_list.append(index)\n",
    "                        os.remove(query_result_next[0]) #delete original downloaded file\n",
    "                        break\n",
    "\n",
    "                    elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                        holes_product_list.append((hole_loc, str(time_data)))\n",
    "                        os.remove(query_result_next[0])\n",
    "                        continue \n",
    "\n",
    "                elif data_next_good is None:\n",
    "                    unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue #continue the for loop\n",
    "\n",
    "\n",
    "    elif (data_product is None) or (axis1_product != axis2_product):\n",
    "        unreadable_file_ids_product_list.append(product_results.get_response(0)[int(indiv_ind)]['fileid'])\n",
    "        bad_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "        all_2hr_sieved_times_product_times.remove(bad_time_val)\n",
    "        ind_bad_time_val = np.where(np.array(all_size_sieved_times) == bad_time_val)[0][0]\n",
    "        all_2hr_sieved_times_product_times_inds_list.remove(ind_bad_time_val)\n",
    "        os.remove(query_result[0]) #delete original downloaded file\n",
    "        ind_timespickup = np.where(np.array(all_size_sieved_times) == bad_time_val)[0][0]\n",
    "        zoomed_time_range = TimeRange(str(bad_time_val),timedelta(hours=time_window))\n",
    "\n",
    "        fetch_inds_to_try_list = [] #gets reset for each new item\n",
    "        for time_val in all_size_sieved_times[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "            if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                ind_next_good_time = np.where(np.array(all_size_sieved_times) == time_val)[0]\n",
    "                fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "        for index in fetch_inds_to_try_list:\n",
    "            query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "            axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "            if data_next_good is not None and axis1_next_good == axis2_next_good:\n",
    "\n",
    "                if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                    reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                    time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                    writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                    all_2hr_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                    all_2hr_sieved_times_product_times_inds_list.append(index)\n",
    "                    os.remove(query_result_next[0])\n",
    "                    break\n",
    "\n",
    "                elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                    time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                    hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                    holes_product_list.append((hole_loc, str(time_data)))\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue \n",
    "\n",
    "            elif data_next_good is None:\n",
    "                unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                os.remove(query_result_next[0])\n",
    "                continue\n",
    "    \n",
    "    all_2hr_sieved_times_product_times_modified = all_2hr_sieved_times_product_times\n",
    "\n",
    "    return all_2hr_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list \n",
    "    #think whether need this new name or need to feed back, i think is ok\n",
    "\n",
    "\n",
    "def csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_2hr_sieved_times_sorted):\n",
    "    with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv', 'a') as f: #appending lines so not overwriting the file\n",
    "        writer = csv.writer(f, delimiter='\\n')\n",
    "        writer.writerow(all_2hr_sieved_times_sorted)\n",
    "\n",
    "\n",
    "def main(date_start, date_finish, target_dimension, time_increment, time_window, flag, home_dir):\n",
    "    \n",
    "    date_time_pre_start = date_start + '-0000'\n",
    "    date_time_start= parser.parse(date_time_pre_start)\n",
    "    print('date_time_start:', date_time_start)\n",
    "\n",
    "    date_time_pre_end = date_finish + '-0000'\n",
    "    date_time_end = parser.parse(date_time_pre_end)\n",
    "    print('date_time_end:', date_time_end)\n",
    "\n",
    "    target_dimension = int(target_dimension)\n",
    "    print('target_dimension:', target_dimension)\n",
    "\n",
    "    time_increment = int(time_increment)\n",
    "    time_window = float(time_window)\n",
    "\n",
    "    flag = str(flag) \n",
    "    print('flag:', flag)\n",
    "\n",
    "    home_dir = str(home_dir)\n",
    "    print('home_dir:', home_dir)\n",
    "\n",
    "    url_prefix = 'https://seal.nascom.nasa.gov/'\n",
    "    print('url_prefix:', url_prefix)\n",
    "\n",
    "    look_ahead = int(np.ceil(time_window*60/10)) #should sufficiently cover all 6 products based on their cadence.\n",
    "    print('look_ahead:', look_ahead)\n",
    "\n",
    "    diff_start_finish_total_sec = (date_time_end - date_time_start).total_seconds()\n",
    "    print('diff_start_finish_total_sec:', diff_start_finish_total_sec)\n",
    "\n",
    "    total_sec = timedelta(days = time_increment).total_seconds()\n",
    "    print('total_sec:', total_sec)\n",
    "\n",
    "    num_loops = np.ceil(diff_start_finish_total_sec/total_sec) #num_loops would be equal to 94 for 19960101-0000' - '20110501-0000' \n",
    "    print('num_loops:', num_loops)\n",
    "    if num_loops != 1:\n",
    "        num_loops += 1\n",
    "        print('num_loops revised:', num_loops)\n",
    "\n",
    "    for base in ['EIT195', 'MDI_96m','LASCO_C2','LASCO_C3','EIT171','EIT304','EIT284']:\n",
    "        holes_list = []\n",
    "        unreadable_file_ids_product_list_global = []\n",
    "    \n",
    "        print(f'***{base}***')\n",
    "        base_dir = home_dir + base\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)    \n",
    "\n",
    "        time_range = TimeRange(date_time_start, timedelta(days = time_increment)) #time_range re-initialized here\n",
    "        #print('time_range:', time_range)\n",
    "\n",
    "        for t_value in np.arange(num_loops): #main workhorse loop\n",
    "            print('t_value:', t_value)    \n",
    "        \n",
    "            product_results = product_search(base,time_range,date_time_start)\n",
    "            product_results_number = product_results.file_num\n",
    "            if product_results_number != 0:\n",
    "                ind = index_of_sizes(base,product_results)\n",
    "                all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, fetch_indices_product = fetch_indices(base,ind,product_results,time_window,look_ahead)\n",
    "                for item in fetch_indices_product:\n",
    "                    query_result = product_retriever(base,product_results,item,url_prefix,home_dir)\n",
    "                    axis1_product,axis2_product,data_product = readfits(query_result[0])\n",
    "                    all_2hr_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list_local = product_distiller(base, axis1_product,axis2_product,data_product, all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, query_result, ind, item, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir)\n",
    "                \n",
    "                    if holes_product_list:\n",
    "                        holes_list.append(holes_product_list)\n",
    "                    \n",
    "                    if unreadable_file_ids_product_list_local:\n",
    "                        unreadable_file_ids_product_list_global.append(unreadable_file_ids_product_list_local)\n",
    "            \n",
    "                all_2hr_sieved_times_sorted = np.sort(all_2hr_sieved_times_product_times_modified)\n",
    "\n",
    "                print(f'{base} all_size_sieved_times:', all_size_sieved_times, len(all_size_sieved_times))\n",
    "                print(f'{base} list(all_2hr_sieved_times_sorted):', list(all_2hr_sieved_times_sorted), len(all_2hr_sieved_times_sorted))\n",
    "            \n",
    "                csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_2hr_sieved_times_sorted)\n",
    "\n",
    "            time_range.next() #Sunpy iterator to go for the next 2 months #also have time_range.previous() to go back. #### UNCOMMENT!    \n",
    "            #print('time_range next:', time_range)\n",
    "        \n",
    "        print(f'{base} holes_list', holes_list)\n",
    "        print(f'{base} unreadable_file_ids_product_list_global:', unreadable_file_ids_product_list_global)\n",
    "\n",
    "        data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension)    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser_args = argparse.ArgumentParser(description='SOHO ML Data experiment parameters')\n",
    "    parser_args.add_argument('--date_start', metavar='time', required=True, help='yyyy-mm-dd, 1996-01-01 is earliest start', type = str)\n",
    "    parser_args.add_argument('--date_finish', metavar='time', required=True, help='yyyy-mm-dd, 2011-05-01 is recommended latest finish, select a minimum range of 2 months', type = str)\n",
    "    parser_args.add_argument('--target_dimension', metavar='image size', required=True, help='e.g., 128', type = int)\n",
    "    parser_args.add_argument('--time_increment', metavar='days at a time to loop over', required=False, help='max time span must be around 2 months as there is a 10k limit to VSO return search query', default = 60, type = int)\n",
    "    parser_args.add_argument('--time_window', metavar='time', required=True, help='time step in hours', type = float)\n",
    "    parser_args.add_argument('--flag', metavar='resize strategy', required=True, help='choose from either \"subsample\", \"interp\", \"minpool\", or \"maxpool\" ', type = str)\n",
    "    parser_args.add_argument('--home_dir', metavar='home directory', required=True, help='str, e.g., \"/home/user/Documents/\", need \"/\" in the end', type = str)\n",
    "\n",
    "\n",
    "    args = parser_args.parse_args()\n",
    "    main(\n",
    "        date_start = args.date_start,\n",
    "        date_finish = args.date_finish,\n",
    "        target_dimension = args.target_dimension,\n",
    "        time_increment = args.time_increment,\n",
    "        time_window = args.time_window,\n",
    "        flag = args.flag,\n",
    "        home_dir = args.home_dir)  \n",
    "        zeros_ind_len = len(zeros_ind)  \n",
    "        \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "\n",
    "    elif 'LASCO_C3' in filename:\n",
    "        #print('LASCO_C3')\n",
    "        rad = 0.8*x_coord\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)  \n",
    "        \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image   \n",
    "\n",
    "    \n",
    "    elif 'LASCO_C2' in filename:\n",
    "        #print('LASCO_C2')\n",
    "        rad1 = 160 #this seems good\n",
    "        #print('rad1:', rad1)\n",
    "        rad2 = int(x_coord)\n",
    "        indices = np.where((rad2**2 > rsquared.flatten()) & (rsquared.flatten() > rad1**2))[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "     \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "        \n",
    "\n",
    "def data_reducer(data,flag,target_dimension,axis1_shape):\n",
    "    scale_factor = int(axis1_shape/target_dimension)\n",
    "    \n",
    "    if flag == 'subsample':\n",
    "        reduced_data = data[::scale_factor].T[::scale_factor].T #subsampling image; every other row,column\n",
    "    elif flag == 'interp': #linear interpolation with anti_aliasing and range preserving\n",
    "        reduced_data = rescale(data, (1/scale_factor), order=1, anti_aliasing=True, preserve_range=True)\n",
    "    elif flag == 'minpool': #min pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.min)\n",
    "    elif flag == 'maxpool': #max pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.max)\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "def data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre) #to have chronological order and to sink order with list of individual product times\n",
    "    print('len(data_files):', len(data_files))\n",
    "    \n",
    "    data_content_list = []\n",
    "    for elem in data_files:\n",
    "        axdim1,axdim2,data_content = readfits(f'{filepath}{elem}')\n",
    "        data_content_list.append(data_content)\n",
    "\n",
    "    data_content_stack = np.stack(data_content_list)\n",
    "    data_cube = h5py.File(f'{home_dir}{date_start}_to_{date_finish}_{base}_{flag}_{target_dimension}.h5', 'w')\n",
    "    data_cube.create_dataset(f'{base}_{target_dimension}', data=data_content_stack, compression=\"gzip\")\n",
    "    data_cube.close()\n",
    "                            \n",
    "    return data_cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_search(base,time_range,date_time_start):\n",
    "    if 'EIT' in base:\n",
    "        wavelen = int(base[3:6])\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('EIT'),avso.Provider('SDAC'),avso.Wavelength(wavelen * avso.u.Angstrom, wavelen * avso.u.Angstrom))\n",
    "    \n",
    "    elif 'MDI' in base:\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('MDI'),avso.Provider('SDAC'),avso.Physobs('LOS_MAGNETIC_FIELD'))\n",
    "    \n",
    "    elif 'LASCO' in base:\n",
    "        detector = base.split('_')[1]\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Provider('SDAC'),avso.Source('SOHO'),avso.Instrument('LASCO'),avso.Detector(detector))\n",
    "    \n",
    "    return product_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_sizes(base,product_results):\n",
    "    \n",
    "    matches = ['171', '304', '284']\n",
    "    \n",
    "    if 'EIT195' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind_2059 = np.where(np.array(size_list) == 2059)[0]\n",
    "        ind_523 = np.where(np.array(size_list) == 523)[0]\n",
    "        print(len(ind_2059))\n",
    "        print(len(ind_523))\n",
    "        ind = np.sort(list(ind_2059) + list(ind_523)) #important to sort here since combining two lists!\n",
    "        print(len(ind))\n",
    "        \n",
    "    elif 'MDI' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 4115.0)[0]\n",
    "        print(len(ind))        \n",
    "        \n",
    "    elif 'LASCO' in base:\n",
    "        size_list = [int(np.ceil(elem['size'] / 100.0))*100 for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2100.0)[0] \n",
    "        print(len(ind))\n",
    "        \n",
    "    elif any([x in base for x in matches]):\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2059)[0]        \n",
    "        print(len(ind))\n",
    "        \n",
    "    return ind\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indices(base,ind,product_results,time_window,look_ahead):\n",
    "    \n",
    "    all_size_sieved_times = [] #local list to populate at each loop\n",
    "    all_2hr_sieved_times = [] #local list to populate at each loop\n",
    "\n",
    "    for value in ind:\n",
    "        all_size_sieved_times.append(product_results.get_response(0)[int(value)]['time']['start'])\n",
    "    all_size_sieved_times_copy = all_size_sieved_times.copy()\n",
    "\n",
    "    for i,time_value in enumerate(all_size_sieved_times_copy):\n",
    "        local_time_range = TimeRange(str(time_value),timedelta(hours=time_window))\n",
    "\n",
    "        local_list = []\n",
    "        for k,time_val in enumerate(all_size_sieved_times_copy[i:i+look_ahead]):\n",
    "            if time_val in local_time_range:\n",
    "                local_list.append(time_val)\n",
    "        if local_list:\n",
    "            for entry in local_list[1:]:\n",
    "                all_size_sieved_times_copy.remove(entry)\n",
    "            all_2hr_sieved_times.append(local_list[0])\n",
    "\n",
    "    all_2hr_sieved_times_product_times = list(np.unique(all_2hr_sieved_times)) #np.unique() does np.array() and np.sort()\n",
    "\n",
    "    all_2hr_sieved_times_product_times_inds_list_pre = [np.where(np.array(all_size_sieved_times) == item)[0] for item in all_2hr_sieved_times_product_times]\n",
    "    \n",
    "    if all_2hr_sieved_times_product_times_inds_list_pre:\n",
    "        all_2hr_sieved_times_product_times_inds_list = list(np.hstack(all_2hr_sieved_times_product_times_inds_list_pre))\n",
    "\n",
    "    fetch_indices_product = ind[all_2hr_sieved_times_product_times_inds_list] #THESE ARE THE INDICES TO FETCH!\n",
    "    \n",
    "    return all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, fetch_indices_product\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_retriever(base,product_results,indiv_ind,url_prefix,home_dir):\n",
    "    \n",
    "    fileid = product_results.get_response(0)[int(indiv_ind)]['fileid']\n",
    "    item_wget =  url_prefix + fileid\n",
    "    cmd = 'wget' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}' #OBTAIN TIMEOUT ISSUE WITH FIDO FETCH! SEEMS THAT WITH WGET CAN CIRCUMNAVIGATE IT.\n",
    "    args = shlex.split(cmd)\n",
    "    subprocess.check_call(args)\n",
    "    downloaded_fileid = fileid.split('/')[-1]\n",
    "    query_result = [f'{home_dir}{base}/{downloaded_fileid}']\n",
    "    \n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_distiller(base, axis1_product,axis2_product,data_product, all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, query_result, ind, indiv_ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir):\n",
    "\n",
    "    holes_product_list = []\n",
    "    unreadable_file_ids_product_list = []\n",
    "    \n",
    "    if (data_product is not None) and (axis1_product == axis2_product):\n",
    "\n",
    "        if not holes(query_result[0]): #so if not True; so no holes; can use image\n",
    "            reduced_product_data = data_reducer(data_product,flag,target_dimension,axis1_product)\n",
    "            time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "            os.remove(query_result[0]) #delete original downloaded file\n",
    "\n",
    "        elif holes(query_result[0]): #so if True, if there are holes\n",
    "            time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start'] \n",
    "            hole_loc = url_prefix + product_results.get_response(0)[int(indiv_ind)]['fileid']                       \n",
    "            holes_product_list.append((hole_loc, str(time_data)))\n",
    "            hole_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            \n",
    "            all_2hr_sieved_times_product_times.remove(hole_time_val)\n",
    "            \n",
    "            ind_hole_time_val = np.where(np.array(all_size_sieved_times) == hole_time_val)[0][0]\n",
    "            \n",
    "            all_2hr_sieved_times_product_times_inds_list.remove(ind_hole_time_val)\n",
    "\n",
    "            os.remove(query_result[0]) #delete original downloaded file\n",
    "            ind_timespickup = np.where(np.array(all_size_sieved_times) == hole_time_val)[0][0]\n",
    "            zoomed_time_range = TimeRange(str(hole_time_val),timedelta(hours=time_window))\n",
    "\n",
    "            fetch_inds_to_try_list = [] \n",
    "            #the zeroth entry didn't have it so that's why plus 1 in the brackets\n",
    "            for time_val in all_size_sieved_times[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                    ind_next_good_time = np.where(np.array(all_size_sieved_times) == time_val)[0]\n",
    "                    fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                    fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "            for index in fetch_inds_to_try_list:\n",
    "                query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                if data_next_good is not None and axis1_next_good == axis2_next_good:\n",
    "\n",
    "                    if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                        reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                        all_2hr_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                        all_2hr_sieved_times_product_times_inds_list.append(index)\n",
    "                        os.remove(query_result_next[0]) #delete original downloaded file\n",
    "                        break\n",
    "\n",
    "                    elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                        holes_product_list.append((hole_loc, str(time_data)))\n",
    "                        os.remove(query_result_next[0])\n",
    "                        continue \n",
    "\n",
    "                elif data_next_good is None:\n",
    "                    unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue #continue the for loop\n",
    "\n",
    "\n",
    "    elif (data_product is None) or (axis1_product != axis2_product):\n",
    "        unreadable_file_ids_product_list.append(product_results.get_response(0)[int(indiv_ind)]['fileid'])\n",
    "        bad_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "        all_2hr_sieved_times_product_times.remove(bad_time_val)\n",
    "        ind_bad_time_val = np.where(np.array(all_size_sieved_times) == bad_time_val)[0][0]\n",
    "        all_2hr_sieved_times_product_times_inds_list.remove(ind_bad_time_val)\n",
    "        os.remove(query_result[0]) #delete original downloaded file\n",
    "        ind_timespickup = np.where(np.array(all_size_sieved_times) == bad_time_val)[0][0]\n",
    "        zoomed_time_range = TimeRange(str(bad_time_val),timedelta(hours=time_window))\n",
    "\n",
    "        fetch_inds_to_try_list = [] #gets reset for each new item\n",
    "        for time_val in all_size_sieved_times[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "            if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                ind_next_good_time = np.where(np.array(all_size_sieved_times) == time_val)[0]\n",
    "                fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "        for index in fetch_inds_to_try_list:\n",
    "            query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "            axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "            if data_next_good is not None and axis1_next_good == axis2_next_good:\n",
    "\n",
    "                if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                    reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                    time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                    writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                    all_2hr_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                    all_2hr_sieved_times_product_times_inds_list.append(index)\n",
    "                    os.remove(query_result_next[0])\n",
    "                    break\n",
    "\n",
    "                elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                    time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                    hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                    holes_product_list.append((hole_loc, str(time_data)))\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue \n",
    "\n",
    "            elif data_next_good is None:\n",
    "                unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                os.remove(query_result_next[0])\n",
    "                continue\n",
    "    \n",
    "    all_2hr_sieved_times_product_times_modified = all_2hr_sieved_times_product_times\n",
    "\n",
    "    return all_2hr_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list \n",
    "    #think whether need this new name or need to feed back, i think is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_2hr_sieved_times_sorted):\n",
    "    with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv', 'a') as f: #appending lines so not overwriting the file\n",
    "        writer = csv.writer(f, delimiter='\\n')\n",
    "        writer.writerow(all_2hr_sieved_times_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def main(date_start, date_finish, target_dimension, time_increment, time_window, flag, home_dir):''' \n",
    "    \n",
    "date_start = '1996-12-01' #'1996-01-01'\n",
    "date_finish = '1996-12-03' #'2011-05-01'\n",
    "target_dimension = 128\n",
    "time_increment = 1 #60\n",
    "time_window = 2 #6\n",
    "flag = 'subsample'\n",
    "home_dir = '/home/carl/Documents/'\n",
    "\n",
    "date_time_pre_start = date_start + '-0000'\n",
    "date_time_start= parser.parse(date_time_pre_start)\n",
    "print('date_time_start:', date_time_start)\n",
    "\n",
    "date_time_pre_end = date_finish + '-0000'\n",
    "date_time_end = parser.parse(date_time_pre_end)\n",
    "print('date_time_end:', date_time_end)\n",
    "\n",
    "target_dimension = int(target_dimension)\n",
    "print('target_dimension:', target_dimension)\n",
    "\n",
    "time_increment = int(time_increment)\n",
    "time_window = float(time_window)\n",
    "\n",
    "flag = str(flag) \n",
    "print('flag:', flag)\n",
    "\n",
    "home_dir = str(home_dir)\n",
    "print('home_dir:', home_dir)\n",
    "\n",
    "url_prefix = 'https://seal.nascom.nasa.gov/'\n",
    "print('url_prefix:', url_prefix)\n",
    "\n",
    "look_ahead = int(np.ceil(time_window*60/10)) #should sufficiently cover all 6 products based on their cadence.\n",
    "print('look_ahead:', look_ahead)\n",
    "\n",
    "diff_start_finish_total_sec = (date_time_end - date_time_start).total_seconds()\n",
    "print('diff_start_finish_total_sec:', diff_start_finish_total_sec)\n",
    "\n",
    "total_sec = timedelta(days = time_increment).total_seconds()\n",
    "print('total_sec:', total_sec)\n",
    "\n",
    "num_loops = np.ceil(diff_start_finish_total_sec/total_sec) #num_loops would be equal to 94 for 19960101-0000' - '20110501-0000' \n",
    "print('num_loops:', num_loops)\n",
    "if num_loops != 1:\n",
    "    num_loops += 1\n",
    "    print('num_loops revised:', num_loops)\n",
    "\n",
    "for base in ['EIT195', 'MDI_96m','LASCO_C2','LASCO_C3','EIT171','EIT304','EIT284']:\n",
    "    holes_list = []\n",
    "    unreadable_file_ids_product_list_global = []\n",
    "    \n",
    "    print(f'***{base}***')\n",
    "    base_dir = home_dir + base\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)    \n",
    "\n",
    "    time_range = TimeRange(date_time_start, timedelta(days = time_increment)) #time_range re-initialized here\n",
    "    #print('time_range:', time_range)\n",
    "\n",
    "    for t_value in np.arange(num_loops): #main workhorse loop\n",
    "        print('t_value:', t_value)    \n",
    "        \n",
    "        product_results = product_search(base,time_range,date_time_start)\n",
    "        product_results_number = product_results.file_num\n",
    "        if product_results_number != 0:\n",
    "            ind = index_of_sizes(base,product_results)\n",
    "            all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, fetch_indices_product = fetch_indices(base,ind,product_results,time_window,look_ahead)\n",
    "            for item in fetch_indices_product:\n",
    "                query_result = product_retriever(base,product_results,item,url_prefix,home_dir)\n",
    "                axis1_product,axis2_product,data_product = readfits(query_result[0])\n",
    "                all_2hr_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list_local = product_distiller(base, axis1_product,axis2_product,data_product, all_size_sieved_times, all_2hr_sieved_times_product_times, all_2hr_sieved_times_product_times_inds_list, query_result, ind, item, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir)\n",
    "                \n",
    "                if holes_product_list:\n",
    "                    holes_list.append(holes_product_list)\n",
    "                    \n",
    "                if unreadable_file_ids_product_list_local:\n",
    "                    unreadable_file_ids_product_list_global.append(unreadable_file_ids_product_list_local)\n",
    "            \n",
    "            all_2hr_sieved_times_sorted = np.sort(all_2hr_sieved_times_product_times_modified)\n",
    "\n",
    "            print(f'{base} all_size_sieved_times:', all_size_sieved_times, len(all_size_sieved_times))\n",
    "            print(f'{base} list(all_2hr_sieved_times_sorted):', list(all_2hr_sieved_times_sorted), len(all_2hr_sieved_times_sorted))\n",
    "            \n",
    "            csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_2hr_sieved_times_sorted)\n",
    "\n",
    "        time_range.next() #Sunpy iterator to go for the next 2 months #also have time_range.previous() to go back. #### UNCOMMENT!    \n",
    "        #print('time_range next:', time_range)\n",
    "        \n",
    "    print(f'{base} holes_list', holes_list)\n",
    "    print(f'{base} unreadable_file_ids_product_list_global:', unreadable_file_ids_product_list_global)\n",
    "\n",
    "    data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension)    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"    \n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser_args = argparse.ArgumentParser(description='SOHO ML Data experiment parameters')\n",
    "    parser_args.add_argument('--date_start', metavar='time', required=True, help='yyyy-mm-dd, 1996-01-01 is earliest start', type = str)\n",
    "    parser_args.add_argument('--date_finish', metavar='time', required=True, help='yyyy-mm-dd, 2011-05-01 is recommended latest finish, select a minimum range of 2 months', type = str)\n",
    "    parser_args.add_argument('--target_dimension', metavar='image size', required=True, help='e.g., 128', type = int)\n",
    "    parser_args.add_argument('--time_increment', metavar='days at a time to loop over', required=False, help='max time span must be around 2 months as there is a 10k limit to VSO return search query', default = 60, type = int)\n",
    "    parser_args.add_argument('--time_window', metavar='time', required=True, help='time step in hours', type = float)\n",
    "    parser_args.add_argument('--flag', metavar='resize strategy', required=True, help='choose from either \"subsample\", \"interp\", \"minpool\", or \"maxpool\" ', type = str)\n",
    "    parser_args.add_argument('--home_dir', metavar='home directory', required=True, help='str, e.g., \"/home/user/Documents/\", need \"/\" in the end', type = str)\n",
    "\n",
    "\n",
    "    args = parser_args.parse_args()\n",
    "    main(\n",
    "        date_start = args.date_start,\n",
    "        date_finish = args.date_finish,\n",
    "        target_dimension = args.target_dimension,\n",
    "        time_increment = args.time_increment,\n",
    "        time_window = args.time_window,\n",
    "        flag = args.flag,\n",
    "        home_dir = args.home_dir)  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
