{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import parser\n",
    "import time\n",
    "from time import process_time\n",
    "\n",
    "from sunpy.net import Fido\n",
    "from sunpy.net.vso import attrs as avso\n",
    "from sunpy.time import TimeRange #parse_time,\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "\n",
    "import h5py\n",
    "\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfits(filename):\n",
    "    try:\n",
    "        ft = fits.open(filename, memmap=False)\n",
    "        hdr = ft[0].header\n",
    "        data = ft[0].data\n",
    "        axis1 = hdr['naxis1']\n",
    "        axis2 = hdr['naxis2']\n",
    "        ft.close()\n",
    "    \n",
    "    except ValueError:\n",
    "        axis1 = 1\n",
    "        axis2 = 2\n",
    "        data = None\n",
    "            \n",
    "    return axis1,axis2,data\n",
    "\n",
    "def writefits(filename, data, home_dir):\n",
    "    if not os.path.exists(f'{home_dir}{filename}.fits'):\n",
    "        fitsname = fits.PrimaryHDU(data)\n",
    "        fitsname.writeto(f'{home_dir}{filename}.fits')\n",
    "\n",
    "def holes(filename):\n",
    "    filename = str(filename)\n",
    "    \n",
    "    ft = fits.open(filename, memmap=False)\n",
    "    hdr = ft[0].header\n",
    "    data = ft[0].data\n",
    "    ft.close()\n",
    "\n",
    "    try:\n",
    "        x_coord = hdr['CRPIX1']\n",
    "        y_coord = hdr['CRPIX2']\n",
    "    \n",
    "    except KeyError:\n",
    "        x_coord = hdr['naxis1'] / 2.\n",
    "        y_coord = hdr['naxis2'] / 2.\n",
    "\n",
    "    y_ind,x_ind = np.indices((hdr['naxis1'],hdr['naxis2']))\n",
    "    rsquared = (x_ind - x_coord)**2 + (y_ind - y_coord)**2\n",
    "    \n",
    "    matches = ['96m', 'MDI']\n",
    "    \n",
    "    if 'efz' in filename: #good for all EIT products \n",
    "        rad = x_coord*np.sqrt(2)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "\n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "    \n",
    "    elif any([x in filename for x in matches]):\n",
    "        rad1 = float(x_coord)\n",
    "        rad2 = 0.6*float(x_coord)\n",
    "        indices_rad1 = np.where(rsquared.flatten() < rad1**2)[0]\n",
    "        indices_rad2 = np.where(rsquared.flatten() < rad2**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices_rad1] == 0.)[0]\n",
    "        nan_ind = np.where(data.flatten()[indices_rad2] != data.flatten()[indices_rad2])[0]\n",
    "        zeros_nan_ind_len = len(list(zeros_ind) + list(nan_ind))\n",
    "        \n",
    "        if zeros_nan_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "\n",
    "    elif 'LASCO_C3' in filename:\n",
    "        #print('LASCO_C3')\n",
    "        rad = 0.8*x_coord\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)  \n",
    "        \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image   \n",
    "\n",
    "    \n",
    "    elif 'LASCO_C2' in filename:\n",
    "        #print('LASCO_C2')\n",
    "        rad1 = 160 #this seems good\n",
    "        #print('rad1:', rad1)\n",
    "        rad2 = int(x_coord)\n",
    "        indices = np.where((rad2**2 > rsquared.flatten()) & (rsquared.flatten() > rad1**2))[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "     \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "        \n",
    "\n",
    "def data_reducer(data,flag,target_dimension,axis1_shape):\n",
    "    scale_factor = int(axis1_shape/target_dimension)\n",
    "    \n",
    "    if flag == 'subsample':\n",
    "        reduced_data = data[::scale_factor].T[::scale_factor].T #subsampling image; every other row,column\n",
    "    elif flag == 'interp': #linear interpolation with anti_aliasing and range preserving\n",
    "        reduced_data = rescale(data, (1/scale_factor), order=1, anti_aliasing=True, preserve_range=True)\n",
    "    elif flag == 'minpool': #min pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.min)\n",
    "    elif flag == 'maxpool': #max pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.max)\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "def prev_time_resumer(home_dir, base, time_range_orig, date_time_end): \n",
    "    #CAN RE-RUN PROGRAM FROM THE LAST DATE ON WHICH STOPPED; WILL PICK UP THE TIMES THAT ARE PRESENT AND CONTINUE!\n",
    "    # CHECKS WHETHER THE START DAY THAT ENTERED IS ALREADY CONTAINED IN THE FILES OF PREVIOUS DAY AND START_DATE FROM THAT EXACT TIME! \n",
    "    # ALSO THIS WORKS IF START ON A NEW DAY AND ARE LOOKING BACK ON THE PREVIOUS DAY.\n",
    "    \n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre)\n",
    "    \n",
    "    if len(data_files) != 0:\n",
    "        prev_time_pre = data_files[-1]\n",
    "        if 'EIT' in str(prev_time_pre):\n",
    "            prev_time = [prev_time_pre.split('_')[2]]\n",
    "        else:\n",
    "            prev_time = [prev_time_pre.split('_')[3]]  \n",
    "            \n",
    "        time_orig_pre = str(time_range_orig.start)\n",
    "        time_orig = ''.join(time_orig_pre.split(' ')[0].split('-'))\n",
    "        \n",
    "        if str(prev_time[0][0:8]) == time_orig:\n",
    "            time_begin = prev_time[0]\n",
    "            time_range = TimeRange(time_begin, date_time_end)\n",
    "        else:\n",
    "            time_range = time_range_orig            \n",
    "    \n",
    "    elif len(data_files) == 0:\n",
    "        prev_time = []\n",
    "        time_range = time_range_orig   \n",
    "    \n",
    "    return prev_time, time_range\n",
    "\n",
    "\n",
    "def data_name_selector(home_dir, base, date_start, date_finish):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre)\n",
    "    print('len(data_files):', len(data_files)) \n",
    "    \n",
    "    if len(data_files) != 0: \n",
    "        time_start_name_pre = data_files[0] \n",
    "        time_finish_name_pre = data_files[-1]\n",
    "        \n",
    "        if 'EIT' in str(time_start_name_pre):\n",
    "            time_start_name = str(time_start_name_pre.split('_')[2])\n",
    "            time_finish_name = str(time_finish_name_pre.split('_')[2])        \n",
    "        else:         \n",
    "            time_start_name = str(time_start_name_pre.split('_')[3])\n",
    "            time_finish_name = str(time_finish_name_pre.split('_')[3])\n",
    "            \n",
    "        time_start_name_new = time_start_name[0:4] + '-' + time_start_name[4:6] + '-' + time_start_name[6:8] + '-' + time_start_name[8:10] + ':' + time_start_name[10:12] + ':' + time_start_name[12:14]\n",
    "        time_finish_name_new = time_finish_name[0:4] + '-' + time_finish_name[4:6] + '-' + time_finish_name[6:8] + '-' + time_finish_name[8:10] + ':' + time_finish_name[10:12] + ':' + time_finish_name[12:14]\n",
    "    \n",
    "    else: \n",
    "        time_start_name_new = date_start \n",
    "        time_finish_name_new = date_finish  \n",
    "        \n",
    "    return time_start_name_new, time_finish_name_new\n",
    "\n",
    "\n",
    "def data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre) #to have chronological order and to sink order with list of individual product times\n",
    "    print('len(data_files):', len(data_files))\n",
    "    \n",
    "    data_content_list = []\n",
    "    for elem in data_files:\n",
    "        axdim1,axdim2,data_content = readfits(f'{filepath}{elem}')\n",
    "        if (axdim1 == axdim2) and ('SOHO' in elem):\n",
    "            data_content_list.append(data_content)\n",
    "\n",
    "    if data_content_list:\n",
    "        data_content_stack = np.stack(data_content_list)\n",
    "    else:\n",
    "        data_content_stack = []\n",
    "    \n",
    "    time_start_name_new, time_finish_name_new = data_name_selector(home_dir, base, date_start, date_finish)\n",
    "    \n",
    "    data_content_stack = np.stack(data_content_list)\n",
    "    data_cube = h5py.File(f'{home_dir}{time_start_name_new}_to_{time_finish_name_new}_{base}_{flag}_{target_dimension}.h5', 'w')\n",
    "    data_cube.create_dataset(f'{base}_{target_dimension}', data=data_content_stack, compression=\"gzip\")\n",
    "    data_cube.close()\n",
    "                            \n",
    "    return data_cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_search(base,time_range,date_time_start):\n",
    "    if 'EIT' in base:\n",
    "        wavelen = int(base[3:6])\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('EIT'),avso.Provider('SDAC'),avso.Wavelength(wavelen * avso.u.Angstrom, wavelen * avso.u.Angstrom))\n",
    "    \n",
    "    elif 'MDI' in base:\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('MDI'),avso.Provider('SDAC'),avso.Physobs('LOS_MAGNETIC_FIELD'))\n",
    "    \n",
    "    elif 'LASCO' in base:\n",
    "        detector = base.split('_')[1]\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Provider('SDAC'),avso.Source('SOHO'),avso.Instrument('LASCO'),avso.Detector(detector))\n",
    "    \n",
    "    return product_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_sizes(base,product_results):\n",
    "    \n",
    "    matches = ['171', '304', '284']\n",
    "    \n",
    "    if 'EIT195' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind_2059 = np.where(np.array(size_list) == 2059)[0]\n",
    "        ind_523 = np.where(np.array(size_list) == 523)[0]\n",
    "        print(len(ind_2059))\n",
    "        print(len(ind_523))\n",
    "        ind = np.sort(list(ind_2059) + list(ind_523)) #important to sort here since combining two lists!\n",
    "        print(len(ind))\n",
    "        \n",
    "    elif 'MDI' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 4115.0)[0]\n",
    "        print(len(ind))        \n",
    "        \n",
    "    elif 'LASCO' in base:\n",
    "        size_list = [int(np.ceil(elem['size'] / 100.0))*100 for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2100.0)[0] \n",
    "        print(len(ind))\n",
    "        \n",
    "    elif any([x in base for x in matches]):\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2059)[0]        \n",
    "        print(len(ind))\n",
    "        \n",
    "    return ind\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indices(base,ind,product_results,time_window,look_ahead,prev_time):\n",
    "    \n",
    "    all_size_sieved_times_pre = [] #local list to populate at each loop\n",
    "    all_time_window_sieved_times = [] #local list to populate at each loop\n",
    "\n",
    "    for value in ind:\n",
    "        all_size_sieved_times_pre.append(product_results.get_response(0)[int(value)]['time']['start'])\n",
    "    all_size_sieved_times = list(np.unique(all_size_sieved_times_pre))\n",
    "    all_size_sieved_times_aug = prev_time + all_size_sieved_times\n",
    "\n",
    "    for i,time_value in enumerate(all_size_sieved_times_aug):\n",
    "        local_time_range = TimeRange(str(time_value),timedelta(hours=time_window))\n",
    "\n",
    "        local_list = []\n",
    "        for k,time_val in enumerate(all_size_sieved_times_aug[i:i+look_ahead]):\n",
    "            if time_val in local_time_range:\n",
    "                local_list.append(time_val)\n",
    "        if local_list:\n",
    "            for entry in local_list[1:]:\n",
    "                all_size_sieved_times_aug.remove(entry)\n",
    "            all_time_window_sieved_times.append(local_list[0])\n",
    "\n",
    "    all_time_window_sieved_times_product_times = list(np.unique(all_time_window_sieved_times)) #np.unique() does np.array() and np.sort()\n",
    "\n",
    "    if not prev_time:\n",
    "        new_inds = [np.where(np.array(all_size_sieved_times_pre) == entry)[0][0] for entry in all_time_window_sieved_times_product_times]           \n",
    "    elif prev_time:\n",
    "        new_inds = [np.where(np.array(all_size_sieved_times_pre) == entry)[0][0] for entry in all_time_window_sieved_times_product_times[1:]]    \n",
    "    \n",
    "    fetch_indices_product = ind[new_inds]\n",
    "    \n",
    "    return all_size_sieved_times_pre, fetch_indices_product\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_retriever(base,product_results,indiv_ind,url_prefix,home_dir):\n",
    "    \n",
    "    fileid = product_results.get_response(0)[int(indiv_ind)]['fileid']\n",
    "    item_wget =  url_prefix + fileid\n",
    "    cmd = 'wget' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}' #OBTAIN TIMEOUT ISSUE WITH FIDO FETCH! SEEMS THAT WITH WGET CAN CIRCUMNAVIGATE IT.\n",
    "    args = shlex.split(cmd)\n",
    "    wget_output = str(subprocess.check_output(args)).strip('b')\n",
    "    \n",
    "    while wget_output != \"''\":\n",
    "        print(f'Encountered wget error with exit status {wget_output} for {item_wget}')\n",
    "        cmd = 'wget' + ' ' + '--retry-connrefused' + ' ' + '--waitretry=1' + ' ' + '--read-timeout=20' + ' ' + '--timeout=15' + ' ' + '-t' + ' ' + '0' + ' ' + '--continue' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}'    \n",
    "        args = shlex.split(cmd)\n",
    "        wget_output = str(subprocess.check_output(args)).strip('b')\n",
    "        if wget_output == \"''\":\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        \n",
    "    downloaded_fileid = fileid.split('/')[-1]\n",
    "    query_result = [f'{home_dir}{base}/{downloaded_fileid}']\n",
    "    \n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_distiller(fetch_indices_product_orig, base, all_size_sieved_times_pre, ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir):\n",
    "    \n",
    "    holes_product_list = []\n",
    "    unreadable_file_ids_product_list = []\n",
    "    \n",
    "    all_time_window_sieved_times_product_times = []\n",
    "    all_time_window_sieved_times_product_times_inds_list = []\n",
    "    \n",
    "    fetch_indices_product = fetch_indices_product_orig.copy()\n",
    "    \n",
    "    for i,elem in enumerate(fetch_indices_product):\n",
    "        indiv_ind = fetch_indices_product[i]\n",
    "        query_result = product_retriever(base,product_results,indiv_ind,url_prefix,home_dir) #item -> indiv_ind is a member of fetch_indices_product\n",
    "        axis1_product, axis2_product, data_product = readfits(query_result[0])\n",
    "    \n",
    "        if (data_product is not None) and (axis1_product == axis2_product):\n",
    "\n",
    "            if not holes(query_result[0]): #so if not True; so no holes; can use image\n",
    "                reduced_product_data = data_reducer(data_product,flag,target_dimension,axis1_product)\n",
    "                time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "                writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "                os.remove(query_result[0]) #delete original downloaded file\n",
    "                all_time_window_sieved_times_product_times.append(time_data)\n",
    "                all_time_window_sieved_times_product_times_inds_list.append(indiv_ind)\n",
    "\n",
    "            elif holes(query_result[0]): #so if True, if there are holes\n",
    "                time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start'] \n",
    "                hole_loc = url_prefix + product_results.get_response(0)[int(indiv_ind)]['fileid']                       \n",
    "                holes_product_list.append((hole_loc, str(time_data)))\n",
    "                hole_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "\n",
    "                ind_hole_time_val = np.where(np.array(all_size_sieved_times_pre) == hole_time_val)[0][0]\n",
    "\n",
    "                os.remove(query_result[0]) #delete original downloaded file\n",
    "                ind_timespickup = np.where(np.array(all_size_sieved_times_pre) == hole_time_val)[0][0]\n",
    "                zoomed_time_range = TimeRange(str(hole_time_val),timedelta(hours=time_window))\n",
    "\n",
    "                fetch_inds_to_try_list = [] \n",
    "                #the zeroth entry didn't have it so that's why plus 1 in the brackets\n",
    "                for time_val in all_size_sieved_times_pre[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                    if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                        ind_next_good_time = np.where(np.array(all_size_sieved_times_pre) == time_val)[0][0]\n",
    "                        fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                        fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "                for index in fetch_inds_to_try_list:\n",
    "                    query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                    axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                    if (data_next_good is not None) and (axis1_next_good == axis2_next_good):\n",
    "\n",
    "                        if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                            reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                            time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                            writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                            all_time_window_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                            all_time_window_sieved_times_product_times_inds_list.append(index)\n",
    "                            os.remove(query_result_next[0]) #delete original downloaded file\n",
    "                            \n",
    "                            indiv_ind_modified_list = []\n",
    "                            localized_time_range = TimeRange(str(time_data),timedelta(hours=time_window)).next() \n",
    "                            print('localized_time_range:', localized_time_range)\n",
    "                            for tval in all_size_sieved_times_pre: \n",
    "                                if parser.parse(tval) < localized_time_range.start:\n",
    "                                    continue \n",
    "                                elif tval in localized_time_range:\n",
    "                                    ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                    indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                    indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                    localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()                                \n",
    "                                else:\n",
    "                                    ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                    indiv_ind_modified_new = ind[ind_time_new]            \n",
    "                                    next_orig_index = np.where(np.array(fetch_indices_product_orig) > np.array(indiv_ind_modified_new))[0]\n",
    "                                    if len(next_orig_index) != 0:\n",
    "                                        indiv_ind_modified_new = fetch_indices_product_orig[next_orig_index[0]]\n",
    "                                        ind_next_index = np.where(np.array(ind) == indiv_ind_modified_new)[0][0]\n",
    "                                        tval = all_size_sieved_times_pre[ind_next_index]\n",
    "                                        indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                        localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "                                    else:\n",
    "                                        ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                        indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                        indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                        localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "                                    \n",
    "                            print('indiv_ind_modified_list:', indiv_ind_modified_list)\n",
    "                            \n",
    "                            if indiv_ind_modified_list:\n",
    "                                fetch_indices_product = list(np.zeros(i+1)) + list(indiv_ind_modified_list) #trick to add zeros to maintain same length as original fetch_indices_product\n",
    "                            else:\n",
    "                                fetch_indices_product = list(np.zeros(i+1)) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                            break\n",
    "\n",
    "                        elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                            if len(fetch_indices_product_orig) > len(fetch_indices_product): \n",
    "                                time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                                hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                                holes_product_list.append((hole_loc, str(time_data)))\n",
    "                                os.remove(query_result_next[0])\n",
    "                                indices_to_go_len = len(np.where(indiv_ind < np.array(fetch_indices_product))[0])\n",
    "                                if indices_to_go_len != 0:\n",
    "                                    fetch_indices_product = list(fetch_indices_product) + list(fetch_indices_product[-indices_to_go_len:]) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                                else:    \n",
    "                                    fetch_indices_product = list(np.zeros(i+1)) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                            else:                        \n",
    "                                time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                                hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                                holes_product_list.append((hole_loc, str(time_data)))\n",
    "                                os.remove(query_result_next[0])\n",
    "                            continue \n",
    "\n",
    "                    elif (data_next_good is None) or (axis1_next_good != axis2_next_good):\n",
    "                        if len(fetch_indices_product_orig) > len(fetch_indices_product):\n",
    "                            indices_to_go_len = len(np.where(indiv_ind < np.array(fetch_indices_product))[0])\n",
    "                            if indices_to_go_len != 0:\n",
    "                                fetch_indices_product = list(fetch_indices_product) + list(fetch_indices_product[-indices_to_go_len:]) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                            else:\n",
    "                                fetch_indices_product = list(np.zeros(i+1)) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))                                \n",
    "                        else:\n",
    "                            unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                            os.remove(query_result_next[0])\n",
    "                        continue\n",
    "\n",
    "\n",
    "        elif (data_product is None) or (axis1_product != axis2_product):\n",
    "            unreadable_file_ids_product_list.append(product_results.get_response(0)[int(indiv_ind)]['fileid'])\n",
    "            bad_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            ind_bad_time_val = np.where(np.array(all_size_sieved_times_pre) == bad_time_val)[0][0]\n",
    "            os.remove(query_result[0])\n",
    "            ind_timespickup = np.where(np.array(all_size_sieved_times_pre) == bad_time_val)[0][0]\n",
    "            zoomed_time_range = TimeRange(str(bad_time_val),timedelta(hours=time_window))\n",
    "\n",
    "            fetch_inds_to_try_list = [] #gets reset for each new item\n",
    "            for time_val in all_size_sieved_times_pre[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                    ind_next_good_time = np.where(np.array(all_size_sieved_times_pre) == time_val)[0][0]\n",
    "                    fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                    fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "            for index in fetch_inds_to_try_list:\n",
    "                query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                axis1_next_good,axis2_next_good,data_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                if (data_next_good is not None) and (axis1_next_good == axis2_next_good):\n",
    "\n",
    "                    if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                        reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                        all_time_window_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                        all_time_window_sieved_times_product_times_inds_list.append(index)\n",
    "                        os.remove(query_result_next[0])\n",
    "                        \n",
    "                        indiv_ind_modified_list = []\n",
    "                        localized_time_range = TimeRange(str(time_data),timedelta(hours=time_window)).next()\n",
    "                        for tval in all_size_sieved_times_pre:\n",
    "                            if parser.parse(tval) < localized_time_range.start:\n",
    "                                continue \n",
    "                            elif tval in localized_time_range:\n",
    "                                ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()                                \n",
    "                            else:\n",
    "                                ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                indiv_ind_modified_new = ind[ind_time_new]   \n",
    "                                next_orig_index = np.where(np.array(fetch_indices_product_orig) > np.array(indiv_ind_modified_new))[0]\n",
    "                                if len(next_orig_index) != 0:\n",
    "                                    indiv_ind_modified_new = fetch_indices_product_orig[next_orig_index[0]]\n",
    "                                    ind_next_index = np.where(np.array(ind) == indiv_ind_modified_new)[0][0]\n",
    "                                    tval = all_size_sieved_times_pre[ind_next_index]\n",
    "                                    indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                    localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "                                else:\n",
    "                                    ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                    indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                    indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                    localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "\n",
    "                        print('indiv_ind_modified_list:', indiv_ind_modified_list)\n",
    "                        \n",
    "                        if indiv_ind_modified_list:\n",
    "                            fetch_indices_product = list(np.zeros(i+1)) + list(indiv_ind_modified_list)\n",
    "                        else:\n",
    "                            fetch_indices_product = list(np.zeros(i+1)) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                        break\n",
    "\n",
    "                    elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                        if len(fetch_indices_product_orig) > len(fetch_indices_product): \n",
    "                            time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                            hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                            holes_product_list.append((hole_loc, str(time_data)))\n",
    "                            os.remove(query_result_next[0])\n",
    "                            indices_to_go_len = len(np.where(indiv_ind < np.array(fetch_indices_product))[0])\n",
    "                            if indices_to_go_len != 0:\n",
    "                                fetch_indices_product = list(fetch_indices_product) + list(fetch_indices_product[-indices_to_go_len:]) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                            else:    \n",
    "                                fetch_indices_product = list(np.zeros(i+1)) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                        else:                        \n",
    "                            time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                            hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                            holes_product_list.append((hole_loc, str(time_data)))\n",
    "                            os.remove(query_result_next[0])\n",
    "                        continue \n",
    "\n",
    "                elif (data_next_good is None) or (axis1_next_good != axis2_next_good):\n",
    "                    if len(fetch_indices_product_orig) > len(fetch_indices_product):\n",
    "                        indices_to_go_len = len(np.where(indiv_ind < np.array(fetch_indices_product))[0])\n",
    "                        if indices_to_go_len != 0:\n",
    "                            fetch_indices_product = list(fetch_indices_product) + list(fetch_indices_product[-indices_to_go_len:]) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))\n",
    "                        else:\n",
    "                            fetch_indices_product = list(np.zeros(i+1)) + list(fetch_indices_product_orig[0]*np.ones(len(fetch_indices_product_orig)))                                \n",
    "                    else:\n",
    "                        unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                        os.remove(query_result_next[0])\n",
    "                    continue\n",
    "    \n",
    "    all_time_window_sieved_times_product_times_modified = all_time_window_sieved_times_product_times\n",
    "\n",
    "    return all_time_window_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keeps first track of all times of all fits files gathered. May contain duplicate times.\n",
    "Hence precautionary step of having csv_time_uniq_writer function.\n",
    "\"\"\"\n",
    "def csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_time_window_sieved_times_sorted):\n",
    "    with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv', 'a') as f: #appending lines so not overwriting the file\n",
    "        writer = csv.writer(f, delimiter='\\n')\n",
    "        writer.writerow(all_time_window_sieved_times_sorted)\n",
    "        \n",
    "\"\"\"\n",
    "The unique times corresponding to all fits files that passed all tests are written to csv files \n",
    "to control for duplicate times in the output csv in order to match the exact number of fits files.\n",
    "\"\"\"\n",
    "def csv_time_uniq_writer(base,home_dir,date_start,date_finish,flag,target_dimension):\n",
    "    if isfile(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv'):    \n",
    "        with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv', 'r') as ff:\n",
    "            csv_reader = csv.reader(ff, delimiter='\\n')\n",
    "            csv_data = [line for line in csv_reader]\n",
    "        csv_uniq_times = list(np.unique(csv_data))\n",
    "        with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}_new.csv', 'w') as g:\n",
    "            writer_new = csv.writer(g, delimiter='\\n')\n",
    "            writer_new.writerow(csv_uniq_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def main(date_start, date_finish, target_dimension, time_increment, time_window, flag, home_dir, bases):''' \n",
    "    \n",
    "date_start = '1996-12-01' #'1996-01-01'\n",
    "date_finish = '1996-12-03' #'2011-05-01'\n",
    "target_dimension = 128\n",
    "time_increment = 1 #60\n",
    "time_window = 2 #6\n",
    "flag = 'subsample'\n",
    "home_dir = '/home/carl/Documents/'\n",
    "bases = 'EIT195, MDI_96m, LASCO_C2, LASCO_C3, EIT171, EIT304, EIT284' #or a subset of these products\n",
    "\n",
    "date_time_pre_start = date_start + '-0000'\n",
    "date_time_start= parser.parse(date_time_pre_start)\n",
    "print('date_time_start:', date_time_start)\n",
    "\n",
    "date_time_pre_end = date_finish + '-2359'\n",
    "date_time_end = parser.parse(date_time_pre_end)\n",
    "print('date_time_end:', date_time_end)\n",
    "\n",
    "target_dimension = int(target_dimension)\n",
    "print('target_dimension:', target_dimension)\n",
    "\n",
    "time_increment = int(time_increment)\n",
    "time_window = float(time_window)\n",
    "\n",
    "flag = str(flag) \n",
    "print('flag:', flag)\n",
    "\n",
    "home_dir = str(home_dir)\n",
    "print('home_dir:', home_dir)\n",
    "\n",
    "url_prefix = 'https://seal.nascom.nasa.gov/'\n",
    "print('url_prefix:', url_prefix)\n",
    "\n",
    "look_ahead = int(np.ceil(time_window*60/10)) #should sufficiently cover all 7 products based on their cadence.\n",
    "print('look_ahead:', look_ahead)\n",
    "\n",
    "diff_start_finish_total_sec = (date_time_end - date_time_start).total_seconds()\n",
    "print('diff_start_finish_total_sec:', diff_start_finish_total_sec)\n",
    "\n",
    "total_sec = timedelta(days = time_increment).total_seconds()\n",
    "print('total_sec:', total_sec)\n",
    "\n",
    "num_loops = np.ceil(diff_start_finish_total_sec/total_sec) + 1 #num_loops would be equal to 94 + 1 for 19960101-0000' - '20110501-0000'; discete number of loops so go over rather than under\n",
    "print('num_loops:', num_loops)\n",
    "\n",
    "base_list = bases.split(',')\n",
    "for base in tqdm(base_list):\n",
    "    \n",
    "    start_process_time = process_time() #initialize clock per product type \n",
    "    \n",
    "    base = base.strip(' ')\n",
    "    holes_list = []\n",
    "    unreadable_file_ids_product_list_global = []\n",
    "    \n",
    "    print(f'***{base}***')\n",
    "    base_dir = home_dir + base\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)    \n",
    "\n",
    "    time_range = TimeRange(date_time_start, timedelta(days = time_increment)) #time_range re-initialized here\n",
    "    #print('time_range:', time_range)\n",
    "\n",
    "    prev_time, time_range_modified = prev_time_resumer(home_dir, base, time_range, date_time_end)\n",
    "    for t_value in tqdm(np.arange(num_loops)): #main workhorse loop\n",
    "        print('t_value:', t_value)\n",
    "        print('prev_time:', prev_time)\n",
    "        \n",
    "        if time_range_modified.end > date_time_end:\n",
    "            time_range_modified = TimeRange(time_range_modified.start, date_time_end)\n",
    "            \n",
    "        product_results = product_search(base,time_range_modified,date_time_start)\n",
    "        product_results_number = product_results.file_num\n",
    "        if product_results_number != 0:\n",
    "            ind = index_of_sizes(base,product_results)\n",
    "            all_size_sieved_times_pre, fetch_indices_product_orig = fetch_indices(base,ind,product_results,time_window,look_ahead,prev_time)          \n",
    "            \n",
    "            if len(fetch_indices_product_orig) != 0:\n",
    "                \n",
    "                all_time_window_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list_local = product_distiller(fetch_indices_product_orig, base, all_size_sieved_times_pre, ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir)\n",
    "                \n",
    "                if holes_product_list:\n",
    "                    holes_list.append(holes_product_list)\n",
    "\n",
    "                if unreadable_file_ids_product_list_local:\n",
    "                    unreadable_file_ids_product_list_global.append(unreadable_file_ids_product_list_local)\n",
    "\n",
    "                all_time_window_sieved_times_sorted = np.unique(all_time_window_sieved_times_product_times_modified)\n",
    "\n",
    "                print(f'{base} np.unique(all_size_sieved_times_pre):', np.unique(all_size_sieved_times_pre), len(np.unique(all_size_sieved_times_pre)))\n",
    "                print(f'{base} list(all_time_window_sieved_times_sorted):', list(all_time_window_sieved_times_sorted), len(all_time_window_sieved_times_sorted))\n",
    "\n",
    "                prev_time = [] #reset to empty list\n",
    "                if len(all_time_window_sieved_times_sorted) != 0:\n",
    "                    prev_time.append(all_time_window_sieved_times_sorted[-1])\n",
    "\n",
    "                csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_time_window_sieved_times_sorted)\n",
    "\n",
    "            else:\n",
    "                holes_list = []\n",
    "                unreadable_file_ids_product_list_global = []\n",
    "        \n",
    "        time_range_modified.next() #Sunpy iterator to go for the next 2 months #also have time_range_modified.previous() to go back.    \n",
    "        #print('time_range_modified next:', time_range_modified)\n",
    "        \n",
    "    print(f'{base} holes_list', holes_list)\n",
    "    print(f'{base} unreadable_file_ids_product_list_global:', unreadable_file_ids_product_list_global)\n",
    "\n",
    "    data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension)\n",
    "    csv_time_uniq_writer(base,home_dir,date_start,date_finish,flag,target_dimension)\n",
    "        \n",
    "    end_process_time = process_time()\n",
    "    time_of_process = end_process_time - start_process_time\n",
    "    print(f'{base} time of process in seconds:', time_of_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
